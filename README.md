# AI貪食蛇遊戲 README

## 專案簡介

本專案是一個利用深度強化學習技術來訓練AI模型玩經典貪食蛇遊戲的應用示例。其主要目標是讓AI能夠自主學習如何有效地通過環境做出決策，以逐步提高貪食蛇的遊戲得分。此專案結合了Python的Pygame庫來呈現遊戲畫面，並使用深度Q網絡（DQN）來實現強化學習。這不僅是一個強化學習的實踐項目，還是一個理解神經網絡如何在動態環境中進行決策的重要教材。

## 目錄

- [專案簡介](#專案簡介)
- [安裝指南](#安裝指南)
- [使用說明](#使用說明)
- [系統需求](#系統需求)
- [功能列表](#功能列表)
- [模型架構](#模型架構)
- [強化學習算法](#強化學習算法)
- [貢獻指南](#貢獻指南)
- [聯絡方式](#聯絡方式)
- [授權](#授權)

## 安裝指南

1. **克隆此專案：**
   ```bash
   git clone https://github.com/你的用戶名/snake-ai-game.git
   cd snake-ai-game
   ```

2. **創建虛擬環境並安裝依賴項：**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # Windows 用戶執行 `venv\Scripts\activate`
   pip install -r requirements.txt
   ```

3. **安裝 PyTorch：**
   
   請參考 [PyTorch 官方網站](https://pytorch.org/get-started/locally/) 選擇適合您平台的安裝命令。

## 使用說明

1. **訓練模型：**
   運行 `train.py` 來開始訓練AI模型：
   ```bash
   python train.py
   ```
   您可以設置不同的訓練參數，例如訓練回合數、學習率、epsilon策略等。以下是一些重要參數：
   - `num_episodes`：訓練的回合數，默認為1000。
   - `batch_size`：每次更新所需的樣本數量，默認為64。
   - `gamma`：折扣因子，用於計算未來獎勵的現在值，默認為0.94。

2. **測試模型：**
   使用訓練好的模型進行遊戲測試：
   ```bash
   python train.py --test
   ```
   在測試模式中，AI將根據訓練好的策略進行遊戲，並顯示最終得分。

3. **遊戲呈現：**
   訓練過程會以視覺方式顯示貪食蛇遊戲畫面，方便觀察AI學習過程中的行為。可以通過設置 `render_mode=True` 來開啟或關閉渲染。

## 系統需求

- Python 3.8 或更高版本
- PyTorch 1.9.0 或更高版本
- Pygame
- Numpy
- TensorBoard（可選，用於監控訓練過程）

## 功能列表

- **強化學習的貪食蛇AI模型：** 使用深度Q網絡（DQN）訓練AI來學習如何自主完成遊戲目標。
- **動態遊戲環境渲染：** 基於Pygame庫的遊戲畫面渲染，幫助可視化AI在訓練過程中的決策和行為。
- **記憶回放（Replay Memory）：** 使用ReplayMemory來存儲遊戲中的狀態轉移，增強模型的訓練效果，讓模型能夠有效地從過去的經驗中學習。
- **TensorBoard訓練監控：** 使用TensorBoard來監控訓練中的損失和獎勵變化，便於對模型的訓練進行分析和調整。
- **雙重DQN（Double DQN）策略：** 透過雙重DQN來避免對Q值的過度估計，提升模型的穩定性和性能。

## 模型架構

本專案使用深度Q網絡（DQN）來進行強化學習，模型包含以下幾層：

- **輸入層：** 接收來自貪食蛇遊戲環境的狀態信息。輸入大小為12，代表遊戲中的各種狀態特徵，例如蛇頭位置、食物位置等。
- **隱藏層：** 使用兩個全連接層，每層包含256個神經元。每個隱藏層使用 ReLU 作為激活函數，以增加模型的非線性表示能力。
- **輸出層：** 輸出每個可能行動的Q值，這些Q值用於評估在當前狀態下選擇各種行動的預期回報。

權重初始化使用 Xavier 初始化技術，以確保每一層的權重分佈在合理的範圍內，從而提高訓練的收斂性。同時，隱藏層中使用 Dropout 以防止過擬合，從而提升模型的泛化能力。

## 強化學習算法

本專案採用深度強化學習中的**深度Q網絡（DQN）**來訓練貪食蛇AI，並結合以下技術策略來提高學習效果：

- **Epsilon-Greedy 策略：** 在訓練過程中使用Epsilon-Greedy策略來平衡探索與利用。隨著訓練的進行，Epsilon逐漸降低，使得AI在前期有更多的探索行為，而後期則更多地利用學到的策略。
- **記憶回放（Replay Memory）：** 通過將每一步的狀態、行動、獎勵和下一狀態存入記憶庫，並在訓練中隨機抽取樣本進行更新，打破數據之間的時間相關性，增強模型的訓練穩定性。
- **雙重DQN（Double DQN）：** 雙重DQN通過引入一個目標網絡來計算Q值，從而有效避免了標準DQN在估計未來回報時可能存在的過度估計問題。這一技術有助於提高模型的穩定性和最終性能。
- **折扣因子（Gamma）：** 折扣因子用於計算未來獎勵的折現值，這樣模型既能考慮短期利益，也能考慮長期策略回報。

## 貢獻指南

我們歡迎所有對該專案有興趣的開發者貢獻自己的力量。以下是如何參與到本專案中的步驟：

1. **Fork 本專案**：首先，點擊GitHub上的“Fork”按鈕來獲取本專案的副本。
2. **創建功能分支**：
   ```bash
   git checkout -b feature/fooBar
   ```
3. **提交更改**：提交您的修改，並加上簡潔的描述：
   ```bash
   git commit -am 'Add some fooBar'
   ```
4. **推送到分支**：將更改推送到您自己的倉庫：
   ```bash
   git push origin feature/fooBar
   ```
5. **發送 Pull Request**：進入GitHub倉庫頁面，點擊“New Pull Request”來提交您的更改。

在提交代碼之前，請確保您的代碼風格一致，並且通過了所有必要的測試。

## 聯絡方式

如果有任何問題或建議，歡迎通過以下方式聯絡我：

- **Email:** your_email@example.com

感謝您的參與！

## 授權

本專案使用 [MIT License](LICENSE) 授權，詳細內容請參見授權文件。
