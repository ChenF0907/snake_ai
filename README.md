# AI 貪食蛇遊戲

## 專案簡介

這個專案是一個 AI 貪食蛇遊戲，使用深度強化學習訓練 AI 自動玩貪食蛇。目標是讓 AI 學會在遊戲中做出最佳決策，逐步提高得分。專案使用 Python 的 Pygame 庫來呈現遊戲畫面，並採用了深度 Q 網絡（DQN）來實現強化學習。

## 目錄

- [專案簡介](#專案簡介)
- [安裝指南](#安裝指南)
- [使用說明](#使用說明)
- [系統需求](#系統需求)
- [功能特色](#功能特色)
- [模型架構](#模型架構)
- [強化學習算法](#強化學習算法)

## 安裝指南

### 1. 克隆這個專案

打開終端機，輸入以下指令來克隆專案：

```bash
git clone https://github.com/ChenF0907/snake-ai-game.git
cd snake-ai-game
```

### 2. 建立虛擬環境並安裝依賴

創建虛擬環境來管理專案依賴：

```bash
python3 -m venv venv
source venv/bin/activate  # Windows 用戶執行 `venv\Scripts\activate`
pip install -r requirements.txt
```

### 3. 安裝 PyTorch

請前往 [PyTorch 官方網站](https://pytorch.org/) 根據系統選擇合適的安裝指令。

## 使用說明

### 1. 訓練 AI 模型

運行以下指令開始訓練 AI 模型：

```bash
python train.py
```

可以調整一些訓練參數，比如訓練回合數、學習率等。重要參數有：

- `num_episodes`：訓練回合數，預設是 1000。
- `batch_size`：每次訓練使用的樣本數，預設是 64。
- `gamma`：折扣因子，用來計算未來獎勵的現在價值，預設是 0.94。

### 2. 測試 AI 模型

訓練完成後，運行以下指令測試 AI 模型：

```bash
python train.py --test
```

AI 會根據訓練好的策略來玩遊戲，並顯示最終得分。

### 3. 遊戲畫面

在訓練過程中，可以看到遊戲畫面，觀察 AI 的決策。設置 `render_mode=True` 可以開啟或關閉畫面渲染。

## 系統需求

- Python 3.8 或更高版本
- PyTorch 1.9.0 或更高版本
- Pygame
- Numpy
- TensorBoard（可選，用來監控訓練過程）

## 功能特色

- **AI 貪食蛇**：使用深度 Q 網絡（DQN）來訓練 AI，自主學習如何在遊戲中得分。
- **動態遊戲畫面**：用 Pygame 呈現遊戲畫面，可以實時觀察 AI 的行為。
- **記憶回放**：儲存遊戲中的經驗，讓 AI 能從過去的經驗中學習，提高訓練效果。
- **訓練監控**：使用 TensorBoard 來監控訓練過程中的損失和獎勵變化。
- **雙重 DQN**：避免 Q 值的過度估計，提升模型的穩定性和性能。

## 模型架構

專案使用深度 Q 網絡（DQN）進行強化學習，模型結構如下：

- **輸入層**：接收遊戲環境的狀態信息，輸入大小為 12，代表蛇的位置、食物的位置等特徵。
- **隱藏層**：兩個全連接層，每層有 256 個神經元，使用 ReLU 激活函數增加非線性。
  - **ReLU 激活函數**：在隱藏層中使用 ReLU 激活函數來增加模型的非線性能力，這樣可以讓模型學習到更複雜的特徵。
  - **Dropout 機制**：在隱藏層中加入 Dropout，可以防止過擬合，讓模型更具泛化能力。
- **輸出層**：輸出每個可能行動的 Q 值，用來評估各種行動的預期回報。
  - **Xavier 初始化**：用 Xavier 初始化權重，這樣可以讓網絡在開始訓練時更加穩定。

## 強化學習算法

專案使用深度 Q 網絡（DQN），結合以下技術來提升學習效果：

- **Epsilon-Greedy 策略**：訓練過程中，AI 會以一定機率隨機選擇行動來探索。
  - **探索與利用平衡**：Epsilon 值開始時較高，代表更多的探索，隨著訓練進行逐漸降低，讓 AI 更專注於已學到的策略。
- **記憶回放**：儲存每一步的狀態、行動、獎勵和下一狀態，隨機抽取樣本來訓練，打破數據的時間相關性。
  - **隨機抽取樣本**：這樣做可以讓模型的訓練更加穩定，不會過於依賴於某些特定的數據序列。
- **雙重 DQN**：引入目標網絡計算 Q 值，避免標準 DQN 在估計未來回報時的過度估計問題，提升模型穩定性和性能。
  - **目標網絡**：每隔一定步數更新一次，這樣可以避免 Q 值的過度估計，使得訓練更加穩定。
- **折扣因子（Gamma）**：用來計算未來獎勵的折現值，讓模型既考慮短期收益，也能考慮長期策略回報。
  - **Gamma 值的作用**：Gamma 的值在 0 到 1 之間，接近 1 代表更加看重長期獎勵，接近 0 則代表只關注短期獎勵。
